---
title: Cloud Credentials
---
import Warning from '@taskcluster/ui/views/Documentation/components/Warning';
import SiteSpecific from '@taskcluster/ui/components/SiteSpecific';

# Cloud Credentials

The Auth service is capable of handing out credentials for cloud services and other external services, with fine-grained permissions.
The permissions of the resulting credentials are governed by Taskcluster scopes.
This capacity can be useful for users of the platform, but is optional.
In cases where no configuration is provided, the corresponding methods will always return errors.

Help your users out by documenting the configured cloud credentials and pointing `ui.site_specific.cloud_credentials_docs_url` to that documentation.

<SiteSpecific>
The documentation for this deployment is at [%cloud_credentials_docs_url%](%cloud_credentials_docs_url%).
</SiteSpecific>

## AWS Credentials

The Auth service's `auth.awsS3Credentials` method distributes STS credentials suitable for access to S3 buckets, governed by scopes.
It takes a bucket name and an access level (read-write or read-only).

The buckets for which this method can return credentials are defined in the optional `auth.aws_credentials_allowed_buckets` Helm property.
This is a JSON value of the form

```json
[
    {
      "accessKeyId": ..,
      "secretAccessKey": ..,
      "buckets": [.., ..],
    }, {
      // ...
    }
]
```

Each object in the JSON value specifies a single AWS access key with the buckets to which it has access.
The user to which this access key corresponds should have an AWS IAM policy like this:

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": "sts:GetFederationToken",
      "Resource": "arn:aws:sts::<account-id-without-hyphens>:federated-user/TemporaryS3ReadWriteCredentials"
    },
    {
      "Sid": "AllowTaskclusterAuthToDelegateAccess",
      "Effect": "Allow",
      "Action": [
        "s3:ListBucket",
        "s3:GetObject",
        "s3:PutObject",
        "s3:DeleteObject",
        "s3:GetBucketLocation"
      ],
      "Resource": [
        "arn:aws:s3:::<bucket>",
        "arn:aws:s3:::<bucket>/*"
      ]
    }
  ]
}
```

With `<account-id-without-hyphens>` replaced with the AWS acount ID and `<bucket>` replaced by the bucket name.
For multiple buckets, just repeat the two elements of the `Resource` array for each bucket.

## Azure Credentials

The Auth service provides credentials for access to Azure Storage tables and containers via methods like `auth.azureTableSAS`.
The inputs to these methods include a storage account name.
Based on the provided scopes, the service will provide credentials for any table or container in that storage account.

The storage accounts for which these methods can return credentials are defined in the optional `auth.azure_accounts` Helm property.
This is a JSON value of the form `{"<accountId>": "<accessKey>"}`.

## GCP Credentials

The Auth service's `auth.gcpCredentials` method distributes credentials for GCP service accounts to callers, governed by scopes.
It takes a GCP project and a service account email.

The projects and accounts for which the service can issue credentials are governed by the optional `auth.gcp_credentials_allowed_projects` Helm property.
This is a JSON string of the form

```json
{
    "project-name": {
        "credentials": {..},
        "allowedServiceAccounts": [..],
    }, ..
}
```

The allowed projects are defined by the keys of the outer object, in this case just `project-name`.
The `credentials` property gives the "key" for a service account in that project that has the "Service Account Token Creator" role.
The `allowedServiceAccounts` property is a list of service account emails in that project for which the `auth` service can distribute credentials.
The API method will reject any requests for unknown projects, or for service accounts in a project that are not listed in `allowedServiceAccounts`.

The "Service Account Token Creator" role allows a service account to create tokens for *all* service account in the project.
The recommended approach is to isolate work into dedicated projects such that this restriction isn't problematic.
It is possible to create more narrowly-focused IAM policies, but this is not currently supported by the GCP console and must be done with manual calls to the GCP `setIamPolicy` API endpoint.

<Warning>
The current implementation only supports one project, with any number of allowed service accounts.
[Future work](https://bugzilla.mozilla.org/show_bug.cgi?id=1599247) will allow multiple projects.
</Warning>

## Sentry Credentials

Taskcluster supports creating credentials for Sentry to support error reporting, via the `auth.sentryDSN` API method.
This method takes a project name as its input value.
This is typically used by workers; other Taskcluster services have these credentials configured via `errorConfig`.

The configuration for this method is in the following Helm properties:

```yaml
auth:
  sentry_auth_token: ...   # a.k.a. API Key
  sentry_organization: ... # Organization into which to place error messages
  sentry_host: ...         # Hostname to which sentry reports should be sent
  sentry_team: ...         # Team to which to assign newly-created projects
  sentry_key_prefix: ...   # Prefix for keys (it's not clear what this means; set it to `taskcluster-auth`)
```

The auth_token must have permissions
* `project:read`
* `project:write`
* `project:admin`
* `org:read`

## Websocktunnel Credentials

Websocktunnel is an external service that workers can use to provide limited inbound access for live-logging and interactive sessions.
Workers request credentials with the `auth.websocktunnelToken` API method.
The Helm property `auth.websocktunnel_secret` is used to generate these credentials and should correspond to a secret configured on the websocktunnel server itself.

## Queue 's3' Artifact Credentials

In order to support the 's3' artifact type, the queue service requires configuration of buckets for public and private artifacts.
In order to operate on these resources the following access policy is needed:

```js
{
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:GetObject",
        "s3:PutObject",
        "s3:DeleteObject"
      ],
      "Resource": [
        "arn:aws:s3:::<public-artifact-bucket>/*"
        "arn:aws:s3:::<private-artifact-bucket>/*"
      ]
    },
    {
      "Effect": "Allow",
      "Action": [
        "s3:GetBucketLocation",
        "s3:ListBucket",
        "s3:PutBucketCORS"
      ],
      "Resource": [
        "arn:aws:s3:::<public-artifact-bucket>"
        "arn:aws:s3:::<private-artifact-bucket>"
      ]
    }
  ]
}
```

Furthermore, you'll need to set the following _bucket policy_ on your public
artifact bucket:
```js
{
  "Statement": [
    {
      "Sid": "PublicReadGetObject",
      "Effect": "Allow",
      "Principal": {
        "AWS": "*"
      },
      "Action": "s3:GetObject",
      "Resource": "arn:aws:s3:::<public-artifact-bucket>/*"
    }
  ]
}
```
